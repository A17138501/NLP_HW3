{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import re\n",
    "import torch\n",
    "import gc\n",
    "from transformers import AutoTokenizer, AutoModelForCausalLM\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from tqdm.notebook import tqdm \n",
    "from time import sleep\n",
    "from sklearn.metrics import mean_absolute_error\n",
    "from huggingface_hub import notebook_login"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Helper function for debugging\n",
    "def dprint(s, debug):\n",
    "    if debug:\n",
    "        print(s)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using device: cpu\n"
     ]
    }
   ],
   "source": [
    "# --- Check Device ---\n",
    "device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
    "print(f\"Using device: {device}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Please log in to your Hugging Face account.\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "526421fa32b942b9bb21129831e8de06",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "VBox(children=(HTML(value='<center> <img\\nsrc=https://huggingface.co/front/assets/huggingface_logo-noborder.svâ€¦"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# --- Model Loading ---\n",
    "# We'll run our models using Hugging Face's transformers library on HPC/ Google Colab/ Lightning.ai\n",
    "# The Llama models are gated, meaning you must request access on their Hugging Face pages.\n",
    "# Once you have access, you need to log in here to download the model weights.\n",
    "\n",
    "# Run this command in your terminal when you are running this notebook for the 1st time\n",
    "# git config --global credential.helper store\n",
    "\n",
    "print(\"Please log in to your Hugging Face account.\")\n",
    "notebook_login()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# This will be our primary model for most of the assignment.\n",
    "model_id_1 = \"meta-llama/Llama-2-7b-chat-hf\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Loading tokenizer for meta-llama/Llama-2-7b-chat-hf...\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "d3e2c97311674756ad7a469b36bc29aa",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "tokenizer_config.json:   0%|          | 0.00/1.62k [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "ced557f120b34cd6bfc316be71414da8",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "tokenizer.model:   0%|          | 0.00/500k [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "6ff1a2c7720649a884e8ad998df55c3d",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "tokenizer.json:   0%|          | 0.00/1.84M [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "e1d62c8be16446c397c73b58a27d62c1",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "special_tokens_map.json:   0%|          | 0.00/414 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading model: meta-llama/Llama-2-7b-chat-hf...\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "7c6330a1b1134fddb1bb69035e67e0a3",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "config.json:   0%|          | 0.00/614 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "ename": "ValueError",
     "evalue": "Using a `device_map`, `tp_plan`, `torch.device` context manager or setting `torch.set_default_device(device)` requires `accelerate`. You can install it with `pip install accelerate`",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[12], line 9\u001b[0m\n\u001b[1;32m      5\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mLoading model: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mmodel_id_1\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m...\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[1;32m      6\u001b[0m \u001b[38;5;66;03m# This downloads the model weights to your environment.\u001b[39;00m\n\u001b[1;32m      7\u001b[0m \u001b[38;5;66;03m# torch_dtype=torch.bfloat16 uses half-precision floats to save memory.\u001b[39;00m\n\u001b[1;32m      8\u001b[0m \u001b[38;5;66;03m# device_map=\"auto\" automatically puts the model on the GPU if available.\u001b[39;00m\n\u001b[0;32m----> 9\u001b[0m model \u001b[38;5;241m=\u001b[39m AutoModelForCausalLM\u001b[38;5;241m.\u001b[39mfrom_pretrained(\n\u001b[1;32m     10\u001b[0m     model_id_1,\n\u001b[1;32m     11\u001b[0m     torch_dtype\u001b[38;5;241m=\u001b[39mtorch\u001b[38;5;241m.\u001b[39mbfloat16,\n\u001b[1;32m     12\u001b[0m     device_map\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mauto\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m     13\u001b[0m )\n\u001b[1;32m     14\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mmodel_id_1\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m model loaded successfully!\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n",
      "File \u001b[0;32m/opt/anaconda3/lib/python3.12/site-packages/transformers/models/auto/auto_factory.py:600\u001b[0m, in \u001b[0;36m_BaseAutoModelClass.from_pretrained\u001b[0;34m(cls, pretrained_model_name_or_path, *model_args, **kwargs)\u001b[0m\n\u001b[1;32m    598\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m model_class\u001b[38;5;241m.\u001b[39mconfig_class \u001b[38;5;241m==\u001b[39m config\u001b[38;5;241m.\u001b[39msub_configs\u001b[38;5;241m.\u001b[39mget(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mtext_config\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;28;01mNone\u001b[39;00m):\n\u001b[1;32m    599\u001b[0m         config \u001b[38;5;241m=\u001b[39m config\u001b[38;5;241m.\u001b[39mget_text_config()\n\u001b[0;32m--> 600\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m model_class\u001b[38;5;241m.\u001b[39mfrom_pretrained(\n\u001b[1;32m    601\u001b[0m         pretrained_model_name_or_path, \u001b[38;5;241m*\u001b[39mmodel_args, config\u001b[38;5;241m=\u001b[39mconfig, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mhub_kwargs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs\n\u001b[1;32m    602\u001b[0m     )\n\u001b[1;32m    603\u001b[0m \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(\n\u001b[1;32m    604\u001b[0m     \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mUnrecognized configuration class \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mconfig\u001b[38;5;241m.\u001b[39m\u001b[38;5;18m__class__\u001b[39m\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m for this kind of AutoModel: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00m\u001b[38;5;28mcls\u001b[39m\u001b[38;5;241m.\u001b[39m\u001b[38;5;18m__name__\u001b[39m\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m.\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m    605\u001b[0m     \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mModel type should be one of \u001b[39m\u001b[38;5;132;01m{\u001b[39;00m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m, \u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;241m.\u001b[39mjoin(c\u001b[38;5;241m.\u001b[39m\u001b[38;5;18m__name__\u001b[39m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mfor\u001b[39;00m\u001b[38;5;250m \u001b[39mc\u001b[38;5;250m \u001b[39m\u001b[38;5;129;01min\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28mcls\u001b[39m\u001b[38;5;241m.\u001b[39m_model_mapping\u001b[38;5;241m.\u001b[39mkeys())\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m.\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m    606\u001b[0m )\n",
      "File \u001b[0;32m/opt/anaconda3/lib/python3.12/site-packages/transformers/modeling_utils.py:311\u001b[0m, in \u001b[0;36mrestore_default_torch_dtype.<locals>._wrapper\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m    309\u001b[0m old_dtype \u001b[38;5;241m=\u001b[39m torch\u001b[38;5;241m.\u001b[39mget_default_dtype()\n\u001b[1;32m    310\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m--> 311\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m func(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n\u001b[1;32m    312\u001b[0m \u001b[38;5;28;01mfinally\u001b[39;00m:\n\u001b[1;32m    313\u001b[0m     torch\u001b[38;5;241m.\u001b[39mset_default_dtype(old_dtype)\n",
      "File \u001b[0;32m/opt/anaconda3/lib/python3.12/site-packages/transformers/modeling_utils.py:4546\u001b[0m, in \u001b[0;36mPreTrainedModel.from_pretrained\u001b[0;34m(cls, pretrained_model_name_or_path, config, cache_dir, ignore_mismatched_sizes, force_download, local_files_only, token, revision, use_safetensors, weights_only, *model_args, **kwargs)\u001b[0m\n\u001b[1;32m   4544\u001b[0m         \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mDeepSpeed Zero-3 is not compatible with passing a `device_map`.\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[1;32m   4545\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m is_accelerate_available():\n\u001b[0;32m-> 4546\u001b[0m         \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(\n\u001b[1;32m   4547\u001b[0m             \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mUsing a `device_map`, `tp_plan`, `torch.device` context manager or setting `torch.set_default_device(device)` \u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m   4548\u001b[0m             \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mrequires `accelerate`. You can install it with `pip install accelerate`\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m   4549\u001b[0m         )\n\u001b[1;32m   4551\u001b[0m \u001b[38;5;66;03m# handling bnb config from kwargs, remove after `load_in_{4/8}bit` deprecation.\u001b[39;00m\n\u001b[1;32m   4552\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m load_in_4bit \u001b[38;5;129;01mor\u001b[39;00m load_in_8bit:\n",
      "\u001b[0;31mValueError\u001b[0m: Using a `device_map`, `tp_plan`, `torch.device` context manager or setting `torch.set_default_device(device)` requires `accelerate`. You can install it with `pip install accelerate`"
     ]
    }
   ],
   "source": [
    "print(f\"\\nLoading tokenizer for {model_id_1}...\")\n",
    "# The tokenizer turns our text prompt into numbers the model can understand.\n",
    "tokenizer = AutoTokenizer.from_pretrained(model_id_1)\n",
    "\n",
    "print(f\"Loading model: {model_id_1}...\")\n",
    "# This downloads the model weights to your environment.\n",
    "# torch_dtype=torch.bfloat16 uses half-precision floats to save memory.\n",
    "# device_map=\"auto\" automatically puts the model on the GPU if available.\n",
    "model = AutoModelForCausalLM.from_pretrained(\n",
    "    model_id_1,\n",
    "    torch_dtype=torch.bfloat16,\n",
    "    device_map=\"auto\"\n",
    ")\n",
    "print(f\"{model_id_1} model loaded successfully!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "def call_model(prompt, student_configs, post_processing_fn, model_obj, tokenizer_obj, debug=False):\n",
    "    \"\"\"\n",
    "    Generates a response using the provided local Hugging Face model and tokenizer.\n",
    "    \"\"\"\n",
    "    # 1. Tokenize the input prompt\n",
    "    inputs = tokenizer_obj(prompt, return_tensors=\"pt\").to(device)\n",
    "\n",
    "    hf_configs = student_configs.copy()\n",
    "    if 'max_tokens' in hf_configs:\n",
    "        # `generate` uses `max_new_tokens` to specify the length of the output\n",
    "        hf_configs['max_new_tokens'] = hf_configs.pop('max_tokens')\n",
    "    if 'stop' in hf_configs:\n",
    "        del hf_configs['stop'] # Stop sequences are handled differently; we'll ignore for simplicity\n",
    "\n",
    "    # 2. Generate output tokens\n",
    "    outputs = model_obj.generate(**inputs, **hf_configs).to(device)\n",
    "    \n",
    "    # 3. Decode the generated tokens back to a string\n",
    "    # We slice the output to only get the newly generated text, not the original prompt\n",
    "    result_new = tokenizer_obj.decode(outputs[0][inputs['input_ids'].shape[1]:], skip_special_tokens=True)\n",
    "    \n",
    "    dprint(\"************ Prompt ************\", debug)\n",
    "    dprint(prompt, debug)\n",
    "    dprint(\"\\n************ Raw Response ************\", debug)\n",
    "    dprint(result_new, debug)\n",
    "\n",
    "    # 4. Apply post-processing to extract the final answer\n",
    "    final_output = post_processing_fn(result_new)\n",
    "    \n",
    "    dprint(\"\\n************ Final Output ************\", debug)\n",
    "    dprint(final_output, debug)\n",
    "\n",
    "    return final_output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_addition_pairs(lower_bound, upper_bound, rng):\n",
    "    \"\"\"Generates two random integers within a specified range.\"\"\"\n",
    "    int_a = int(np.ceil(rng.uniform(lower_bound, upper_bound)))\n",
    "    int_b = int(np.ceil(rng.uniform(lower_bound, upper_bound)))\n",
    "    return int_a, int_b\n",
    "\n",
    "def test_range(added_prompt, prompt_configs, rng, model_obj, tokenizer_obj, n_sample=30,\n",
    "               lower_bound=1, upper_bound=10, fixed_pairs=None,\n",
    "               pre_processing=lambda x:x, post_processing=lambda y:y,\n",
    "               debug=False):\n",
    "    \"\"\"\n",
    "    Tests a language model's addition performance over a range of numbers.\n",
    "\n",
    "    Args:\n",
    "        added_prompt (tuple): A tuple containing the prefix and suffix for the prompt.\n",
    "        prompt_configs (dict): Configuration parameters for the model's generate function.\n",
    "        rng (numpy.random.Generator): A random number generator instance.\n",
    "        model_obj (transformers.PreTrainedModel): The loaded Hugging Face model object.\n",
    "        tokenizer_obj (transformers.PreTrainedTokenizer): The loaded Hugging Face tokenizer object.\n",
    "        n_sample (int): The number of random pairs to generate if fixed_pairs is None.\n",
    "        lower_bound (int): The lower bound for number generation.\n",
    "        upper_bound (int): The upper bound for number generation.\n",
    "        fixed_pairs (list, optional): A list of specific integer tuples to test.\n",
    "        pre_processing (function): A function to apply to the input string before prompting.\n",
    "        post_processing (function): A function to extract the integer answer from the model's output.\n",
    "        debug (bool): If True, prints detailed debugging information.\n",
    "\n",
    "    Returns:\n",
    "        dict: A dictionary containing performance metrics (res, acc, mae, prompt_length).\n",
    "    \"\"\"\n",
    "    # --- Lists for storing results ---\n",
    "    int_as = []\n",
    "    int_bs = []\n",
    "    answers = []\n",
    "    model_responses = []\n",
    "    correct = []\n",
    "    prompts = []\n",
    "    \n",
    "    # --- Determine the test cases ---\n",
    "    iterations = range(n_sample) if fixed_pairs is None else fixed_pairs\n",
    "    \n",
    "    for v in tqdm(iterations):\n",
    "        if fixed_pairs is None:\n",
    "            # Generate two new numbers if no fixed pairs are provided\n",
    "            int_a, int_b = get_addition_pairs(lower_bound=lower_bound, upper_bound=upper_bound, rng=rng)\n",
    "        else:\n",
    "            # Use the provided fixed pairs\n",
    "            int_a, int_b = v\n",
    "            \n",
    "        # --- Construct the prompt for two numbers ---\n",
    "        fixed_prompt = f'{int_a}+{int_b}'\n",
    "        fixed_prompt = pre_processing(fixed_prompt)\n",
    "        \n",
    "        prefix, suffix = added_prompt\n",
    "        prompt = prefix + fixed_prompt + suffix\n",
    "        \n",
    "        # --- Get the model's response ---\n",
    "        model_response = call_model(prompt, prompt_configs, post_processing, model_obj, tokenizer_obj, debug=debug)\n",
    "        \n",
    "        # --- Calculate the correct answer for two numbers ---\n",
    "        answer = int_a + int_b\n",
    "        \n",
    "        # --- Append all results for analysis ---\n",
    "        int_as.append(int_a)\n",
    "        int_bs.append(int_b)\n",
    "        prompts.append(prompt)\n",
    "        answers.append(answer)\n",
    "        model_responses.append(model_response)\n",
    "        correct.append((answer == model_response))\n",
    "        sleep(0.1)\n",
    "\n",
    "    # --- Create a DataFrame to display the results for two numbers ---\n",
    "    df = pd.DataFrame({\n",
    "        'int_a': int_as, \n",
    "        'int_b': int_bs, \n",
    "        'prompt': prompts, \n",
    "        'answer': answers, \n",
    "        'response': model_responses, \n",
    "        'correct': correct\n",
    "    })\n",
    "    print(df)\n",
    "    \n",
    "    # --- Calculate and return performance metrics ---\n",
    "    mae = mean_absolute_error(df['answer'], df['response'])\n",
    "    acc = df.correct.sum() / len(df)\n",
    "    prompt_length = len(prefix) + len(suffix)\n",
    "    res = acc * (1 / prompt_length) * (1 - mae / (1 * 10**4))\n",
    "    \n",
    "    return {'res': res, 'acc': acc, 'mae': mae, 'prompt_length': prompt_length}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "###  Part 1. Zero Shot Addition"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Example: Zero-shot single-digit addition**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# All of this remains the same\n",
    "added_prompt = ('Question: What is ', '?\\\\nAnswer: ')\n",
    "prompt_config = {'max_tokens': 2,\n",
    "                'temperature': 0.7,\n",
    "                'top_k': 50,\n",
    "                'top_p': 0.6,\n",
    "                'repetition_penalty': 1,\n",
    "                'stop': []}\n",
    "\n",
    "def your_pre_processing(input_string):\n",
    "    return input_string\n",
    "\n",
    "def your_post_processing(output_string):\n",
    "    only_digits = re.sub(r\"\\D\", \"\", output_string)\n",
    "    try:\n",
    "        res = int(only_digits)\n",
    "    except:\n",
    "        res = 0\n",
    "    return res\n",
    "\n",
    "# The model name string is no longer passed to the function\n",
    "# It was used in the previous cell to load the 'model' and 'tokenizer' objects\n",
    "print(f\"Testing model: {model_id_1}\")\n",
    "seed = 0\n",
    "rng = np.random.default_rng(seed)\n",
    "\n",
    "# This is the only line that changes\n",
    "res = test_range(\n",
    "    added_prompt=added_prompt,\n",
    "    prompt_configs=prompt_config,\n",
    "    rng=rng,\n",
    "    model_obj=model, \n",
    "    tokenizer_obj=tokenizer,\n",
    "    n_sample=10,\n",
    "    lower_bound=1,\n",
    "    upper_bound=10,\n",
    "    fixed_pairs=None,\n",
    "    pre_processing=your_pre_processing,\n",
    "    post_processing=your_post_processing,\n",
    "    debug=False\n",
    ")\n",
    "print(res)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Example: Zero-shot 7-digit addition**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "prompt_config['max_tokens'] = 8\n",
    "rng = np.random.default_rng(seed)\n",
    "\n",
    "# The call to test_range is updated to pass the model and tokenizer objects.\n",
    "res = test_range(\n",
    "    added_prompt=added_prompt, \n",
    "    prompt_configs=prompt_config, \n",
    "    rng=rng, \n",
    "    model_obj=model,              # Pass the loaded model object\n",
    "    tokenizer_obj=tokenizer,      # Pass the loaded tokenizer object\n",
    "    n_sample=10, \n",
    "    lower_bound=1000000, \n",
    "    upper_bound=9999999, \n",
    "    fixed_pairs=None, \n",
    "    pre_processing=your_pre_processing, \n",
    "    post_processing=your_post_processing, \n",
    "    debug=False\n",
    ")\n",
    "\n",
    "print(res)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "-----------"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Q1a.** In your opinion, what are some factors that cause language model performance to deteriorate from 1 digit to 7 digits?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Answer: "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "-----------"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Q1b**. Play around with the config parameters ('max_tokens','temperature','top_k','top_p','repetition_penalty')\n",
    "* What does each parameter represent?\n",
    "* How does increasing each parameter change the generation?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Answer: "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Q1c. Do 7-digit addition with Qwen3 8B.\n",
    "\n",
    "* How does the performance change?\n",
    "* What are some factors that cause this change?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# --- Before loading Qwen 3, offload Llama 2 to free up VRAM ---\n",
    "\n",
    "# 1. Delete the model and tokenizer variables from memory.\n",
    "# Replace 'model' and 'tokenizer' with the actual variable names you used for Llama 2.\n",
    "del model\n",
    "del tokenizer\n",
    "\n",
    "# 2. Run Python's garbage collector and empty PyTorch's CUDA cache.\n",
    "# This is the crucial step to actually release the GPU memory.\n",
    "gc.collect()\n",
    "torch.cuda.empty_cache()\n",
    "\n",
    "print(\"Llama 2 model offloaded and GPU memory cleared.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# --- Load Qwen 3 8B ---\n",
    "# This is a different model, so we need to load its specific tokenizer and weights.\n",
    "model_id_2 = \"Qwen/Qwen3-8B\"\n",
    "\n",
    "print(f\"\\nLoading tokenizer for {model_id_2}...\")\n",
    "tokenizer_2 = AutoTokenizer.from_pretrained(model_id_2)\n",
    "\n",
    "print(f\"Loading model: {model_id_2}...\")\n",
    "model_2 = AutoModelForCausalLM.from_pretrained(\n",
    "    model_id_2,\n",
    "    torch_dtype=torch.bfloat16,\n",
    "    device_map=\"auto\"\n",
    ")\n",
    "print(f\"{model_id_2} model loaded successfully!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# --- Test on 7-digit addition ---\n",
    "prompt_config['max_tokens'] = 8\n",
    "\n",
    "print(prompt_config)\n",
    "rng = np.random.default_rng(seed)\n",
    "res_2 = test_range(\n",
    "    added_prompt=added_prompt, \n",
    "    prompt_configs=prompt_config, \n",
    "    rng=rng, \n",
    "    model_obj=model_2,              # Pass the loaded model object\n",
    "    tokenizer_obj=tokenizer_2,      # Pass the loaded tokenizer object\n",
    "    n_sample=10, \n",
    "    lower_bound=1000000, \n",
    "    upper_bound=9999999, \n",
    "    fixed_pairs=None, \n",
    "    pre_processing=your_pre_processing, \n",
    "    post_processing=your_post_processing, \n",
    "    debug=False\n",
    ")\n",
    "print(res_2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "-----------"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Answer: "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "-----------"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Q1d.** Here we're giving our language model the prior that the sum of two 7-digit numbers must have a maximum of 8 digits. (by setting max_token=8). What if we remove this prior by increasing the max_token to 20? \n",
    "* Does the model perform well?\n",
    "* What are some reasons why?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Answer: "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "prompt_config['max_tokens'] = 20\n",
    "rng = np.random.default_rng(seed)\n",
    "res_2 = test_range(\n",
    "    added_prompt=added_prompt, \n",
    "    prompt_configs=prompt_config, \n",
    "    rng=rng, \n",
    "    model_obj=model_2,              # Pass the loaded model object\n",
    "    tokenizer_obj=tokenizer_2,      # Pass the loaded tokenizer object\n",
    "    n_sample=10, \n",
    "    lower_bound=1000000, \n",
    "    upper_bound=9999999, \n",
    "    fixed_pairs=None, \n",
    "    pre_processing=your_pre_processing, \n",
    "    post_processing=your_post_processing, \n",
    "    debug=False\n",
    ")\n",
    "print(res_2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 1. Delete the model and tokenizer variables from memory.\n",
    "# Replace 'model' and 'tokenizer' with the actual variable names you used for Llama 2.\n",
    "del model_2\n",
    "del tokenizer_2\n",
    "\n",
    "# 2. Run Python's garbage collector and empty PyTorch's CUDA cache.\n",
    "# This is the crucial step to actually release the GPU memory.\n",
    "gc.collect()\n",
    "torch.cuda.empty_cache()\n",
    "\n",
    "print(f\"{model_id_2} offloaded and GPU memory cleared.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Part 2. In Context Learning\n",
    "\n",
    "We will try to improve the performance of 7-digit addition via in-context learning.\n",
    "We will use [llama-2-7b]. Below is a simple example."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(f\"\\nLoading tokenizer for {model_id_1}...\")\n",
    "# The tokenizer turns our text prompt into numbers the model can understand.\n",
    "tokenizer = AutoTokenizer.from_pretrained(model_id_1)\n",
    "\n",
    "print(f\"Loading model: {model_id_1}...\")\n",
    "# This downloads the model weights to your environment.\n",
    "# torch_dtype=torch.bfloat16 uses half-precision floats to save memory.\n",
    "# device_map=\"auto\" automatically puts the model on the GPU if available.\n",
    "model = AutoModelForCausalLM.from_pretrained(\n",
    "    model_id_1,\n",
    "    torch_dtype=torch.bfloat16,\n",
    "    device_map=\"auto\"\n",
    ")\n",
    "print(f\"{model_id_1} model loaded successfully!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "added_prompt = ('Question: What is 3+7?\\nAnswer: 10\\n Question: What is ', '?\\nAnswer: ') # Question: What is a+b?\\nAnswer:\n",
    "prompt_config = {'max_tokens': 8,\n",
    "                'temperature': 0.7,\n",
    "                'top_k': 50,\n",
    "                'top_p': 0.6,\n",
    "                'repetition_penalty': 1,\n",
    "                'stop': []}\n",
    "rng = np.random.default_rng(seed)\n",
    "res = test_range(\n",
    "    added_prompt=added_prompt, \n",
    "    prompt_configs=prompt_config, \n",
    "    rng=rng, \n",
    "    model_obj=model,              # Pass the loaded model object\n",
    "    tokenizer_obj=tokenizer,      # Pass the loaded tokenizer object\n",
    "    n_sample=10, \n",
    "    lower_bound=1000000, \n",
    "    upper_bound=9999999, \n",
    "    fixed_pairs=None, \n",
    "    pre_processing=your_pre_processing, \n",
    "    post_processing=your_post_processing, \n",
    "    debug=False\n",
    ")\n",
    "\n",
    "print(res)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Q2a**.\n",
    "* How does the performance change with the baseline in-context learning prompt? (compare with \"Example: Zero-shot 7-digit addition\" in Q1)\n",
    "* What are some factors that cause this change?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Answer: "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "-----------"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now we will remove the prior on output length and re-evaluate the performance of our baseline one-shot learning prompt. We need to modify our post processing function to extract the answer from the output sequence. In this case, it is the number in the first line that starts with \"Answer: \"."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Q2b**.\n",
    "* Modify the post processing function\n",
    "* How does the performance change when we relax the output length constraint? (compare with Q2a)\n",
    "* What are some factors that cause this change?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Answer: "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Write your updated post processing function here\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "prompt_config['max_tokens'] = 50 # changed from 8, assuming we don't know the output length\n",
    "                \n",
    "rng = np.random.default_rng(seed)\n",
    "res = test_range(\n",
    "    added_prompt=added_prompt, \n",
    "    prompt_configs=prompt_config, \n",
    "    rng=rng, \n",
    "    model_obj=model,              # Pass the loaded model object\n",
    "    tokenizer_obj=tokenizer,      # Pass the loaded tokenizer object\n",
    "    n_sample=10, \n",
    "    lower_bound=1000000, \n",
    "    upper_bound=9999999, \n",
    "    fixed_pairs=None, \n",
    "    pre_processing=your_pre_processing, \n",
    "    post_processing=your_post_processing, \n",
    "    debug=False\n",
    ")\n",
    "print(res)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "-----------"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Q2c.** Let's change our one-shot learning example to something more \"in-distribution\". Previously we were using 1-digit addition as an example. Let's change it to 7-digit addition (1234567+1234567=2469134). \n",
    "* Evaluate the performance with max_tokens = 8.\n",
    "* Evaluate the performance with max_tokens = 50.\n",
    "* How does the performance change from 1-digit example to 7-digit example?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Answer: "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "prompt_config['max_tokens'] = 8 \n",
    "added_prompt = ('Question: What is 1234567+1234567?\\nAnswer: 2469134\\nQuestion: What is ', '?\\nAnswer: ') # Question: What is a+b?\\nAnswer:\n",
    "res = test_range(\n",
    "    added_prompt=added_prompt, \n",
    "    prompt_configs=prompt_config, \n",
    "    rng=rng, \n",
    "    model_obj=model,              # Pass the loaded model object\n",
    "    tokenizer_obj=tokenizer,      # Pass the loaded tokenizer object\n",
    "    n_sample=10, \n",
    "    lower_bound=1000000, \n",
    "    upper_bound=9999999, \n",
    "    fixed_pairs=None, \n",
    "    pre_processing=your_pre_processing, \n",
    "    post_processing=your_post_processing, \n",
    "    debug=False\n",
    ")\n",
    "print(res)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "prompt_config['max_tokens'] = 50 \n",
    "res = test_range(\n",
    "    added_prompt=added_prompt, \n",
    "    prompt_configs=prompt_config, \n",
    "    rng=rng, \n",
    "    model_obj=model,              # Pass the loaded model object\n",
    "    tokenizer_obj=tokenizer,      # Pass the loaded tokenizer object\n",
    "    n_sample=10, \n",
    "    lower_bound=1000000, \n",
    "    upper_bound=9999999, \n",
    "    fixed_pairs=None, \n",
    "    pre_processing=your_pre_processing, \n",
    "    post_processing=your_post_processing, \n",
    "    debug=False\n",
    ")\n",
    "print(res)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "-----------"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Q2d.** Let's look at a specific example with large absolute error. \n",
    "* Run the cell at least 5 times. Does the error change each time? Why?\n",
    "* Can you think of a prompt to reduce the error?\n",
    "* Why do you think it would work?\n",
    "* Does it work in practice? Why or why not?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Answer:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_range(\n",
    "    added_prompt=added_prompt, \n",
    "    prompt_configs=prompt_config, \n",
    "    rng=rng, \n",
    "    fixed_pairs=[(9090909,1010101)], \n",
    "    pre_processing=your_pre_processing, \n",
    "    post_processing=your_post_processing, \n",
    "    model_obj=model, \n",
    "    tokenizer_obj=tokenizer, \n",
    "    debug=True\n",
    ")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
